# -*- coding: utf-8 -*-
"""Copy of  Multimodal Transformers_Arabic_Data_Any_mode_9_May.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PCTzEJVAf0KfqrFs11mh5zmYW1aIZVof
"""

!pip install multimodal-transformers
!pip install transformers[torch]
!pip install numpy==1.24.0

from dataclasses import dataclass, field
import json
import logging
import os
from typing import Optional

import numpy as np
import pandas as pd
from transformers import (
    AutoTokenizer,
    AutoConfig,
    Trainer,
    EvalPrediction,
    set_seed
)
from transformers.training_args import TrainingArguments

from multimodal_transformers.data import load_data_from_folder
from multimodal_transformers.model import TabularConfig
from multimodal_transformers.model import AutoModelWithTabular

logging.basicConfig(level=logging.INFO)
os.environ['COMET_MODE'] = 'DISABLED'

import re
from sklearn import preprocessing

data_df = pd.read_csv("/content/INNER_JOIN.csv")
# data_df = data_df.iloc[7000:8000,:]

data_df['is_retweet'] = data_df['is_retweet'].fillna(False)
data_df['place'] = data_df['place'].fillna('NOT_APPLICABLE')
data_df['language'] = data_df['language'].fillna('ar')
data_df['tweet_language'] = data_df['tweet_language'].fillna('ar')

data_df['retweet_count'] = data_df['retweet_count'].fillna(0)
data_df['like_count'] = data_df['like_count'].fillna(0)
data_df['reply_count'] = data_df['reply_count'].fillna(0)


data_df.dropna()

le = preprocessing.LabelEncoder()
data_df['language'] = le.fit_transform(data_df['language'])



for i in range(len(data_df['tweet_text'])):
  data_df['tweet_text'][i:i+1] = re.sub(r'[^0-9\u0600-\u06ff\u0750-\u077f\ufb50-\ufbc1\ufbd3-\ufd3f\ufd50-\ufd8f\ufd50-\ufd8f\ufe70-\ufefc\uFDF0-\uFDFD.0-9]+', '' , str(data_df['tweet_text'][i:i+1]))

from sklearn import preprocessing

le = preprocessing.LabelEncoder()
data_df['LABEL'] = le.fit_transform(data_df['LABEL'])

train_df, val_df, test_df = np.split(data_df.sample(frac=1), [int(.8*len(data_df)), int(.9 * len(data_df))])
print('Num examples train-val-test')
print(len(train_df), len(val_df), len(test_df))


train_numerical_cols=[]
numerical_cols = ['retweet_count',	'like_count' ,'reply_count']
for item in numerical_cols:
  train_numerical_cols.append([train_df[item].values])


val_numerical_cols=[]
numerical_cols = ['retweet_count',	'like_count' ,'reply_count']
for item in numerical_cols:
  val_numerical_cols.append([val_df[item].values])



test_numerical_cols=[]
numerical_cols = ['retweet_count',	'like_count' ,'reply_count']
for item in numerical_cols:
  test_numerical_cols.append([test_df[item].values])



print("Train",train_df['LABEL'].value_counts())
print("Validation",val_df['LABEL'].value_counts())
print("Testing",test_df['LABEL'].value_counts())


train_df.to_csv('train.csv')
val_df.to_csv('val.csv')
test_df.to_csv('test.csv')





x = np.vstack(train_numerical_cols)
train_numerical_cols = np.column_stack([x[0],x[1],x[2]])

x = np.vstack(val_numerical_cols)
val_numerical_cols = np.column_stack([x[0],x[1],x[2]])

x = np.vstack(test_numerical_cols)
test_numerical_cols = np.column_stack([x[0],x[1],x[2]])

# Make sure you have git-lfs installed (https://git-lfs.com)
#!git lfs install
#!git clone https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment

# if you want to clone without large files â€“ just their pointers
# prepend your git clone with the following env var:
#!GIT_LFS_SKIP_SMUDGE=1

@dataclass
class ModelArguments:
  """
  Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
  """

  model_name_or_path: str = field(
      metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
  )
  config_name: Optional[str] = field(
      default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
  )
  tokenizer_name: Optional[str] = field(
      default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
  )
  cache_dir: Optional[str] = field(
      default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
  )


@dataclass
class MultimodalDataTrainingArguments:
  """
  Arguments pertaining to how we combine tabular features
  Using `HfArgumentParser` we can turn this class
  into argparse arguments to be able to specify them on
  the command line.
  """

  data_path: str = field(metadata={
                            'help': 'the path to the csv file containing the dataset'
                        })
  column_info_path: str = field(
      default=None,
      metadata={
          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'
  })

  column_info: dict = field(
      default=None,
      metadata={
          'help': 'a dict referencing the text, categorical, numerical, and label columns'
                  'its keys are text_cols, num_cols, cat_cols, and label_col'
  })

  categorical_encode_type: str = field(default='ohe',
                                        metadata={
                                            'help': 'sklearn encoder to use for categorical data',
                                            'choices': ['ohe', 'binary', 'label', 'none']
                                        })
  numerical_transformer_method: str = field(default='yeo_johnson',
                                            metadata={
                                                'help': 'sklearn numerical transformer to preprocess numerical data',
                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']
                                            })
  task: str = field(default="classification",
                    metadata={
                        "help": "The downstream training task",
                        "choices": ["classification", "regression"]
                    })

  mlp_division: int = field(default=4,
                            metadata={
                                'help': 'the ratio of the number of '
                                        'hidden dims in a current layer to the next MLP layer'
                            })
  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',
                                    metadata={
                                        'help': 'method to combine categorical and numerical features, '
                                                'see README for all the method'
                                    })
  mlp_dropout: float = field(default=0.1,
                              metadata={
                                'help': 'dropout ratio used for MLP layers'
                              })
  numerical_bn: bool = field(default=True,
                              metadata={
                                  'help': 'whether to use batchnorm on numerical features'
                              })
  use_simple_classifier: str = field(default=True,
                                      metadata={
                                          'help': 'whether to use single layer or MLP as final classifier'
                                      })
  mlp_act: str = field(default='relu',
                        metadata={
                            'help': 'the activation function to use for finetuning layers',
                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']
                        })
  gating_beta: float = field(default=0.2,
                              metadata={
                                  'help': "the beta hyperparameters used for gating tabular data "
                                          "see https://www.aclweb.org/anthology/2020.acl-main.214.pdf"
                              })

  def __post_init__(self):
      assert self.column_info != self.column_info_path
      if self.column_info is None and self.column_info_path:
          with open(self.column_info_path, 'r') as f:
              self.column_info = json.load(f)

text_cols = ['tweet_text']
cat_cols = ['place',"language","tweet_language",'is_retweet']
#cat_cols = ['is_retweet']
numerical_cols = ['retweet_count',	'like_count' ,'reply_count']

column_info_dict = {
    'text_cols': text_cols,
    'num_cols': numerical_cols,
    'cat_cols': cat_cols,
    'label_col': 'LABEL',
    'label_list': [1 , 0]
}


model_args = ModelArguments(
    #model_name_or_path='Davlan/bert-base-multilingual-cased-ner-hrl'
    model_name_or_path='Davlan/bert-base-multilingual-cased-ner-hrl'
)

data_args = MultimodalDataTrainingArguments(
    data_path='/content/',
    combine_feat_method='concat',
    column_info=column_info_dict,
    task='classification'
)

training_args = TrainingArguments(
    output_dir="/content/logs/model_name",
    logging_dir="/content/logs/runs",
    overwrite_output_dir=True,
    do_train=True,
    do_eval=True,
    per_device_train_batch_size=50,
    num_train_epochs=15,
    evaluation_strategy='epoch',
    logging_steps=25,
    eval_steps=20
)

set_seed(training_args.seed)

tokenizer_path_or_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path
print('Specified tokenizer: ', tokenizer_path_or_name)
tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path_or_name,
    cache_dir=model_args.cache_dir,
)

# Get Datasets
train_dataset, val_dataset, test_dataset = load_data_from_folder(
    data_args.data_path,
    data_args.column_info['text_cols'],
    tokenizer,
    label_col=data_args.column_info['label_col'],
    label_list=data_args.column_info['label_list'],
    categorical_cols=data_args.column_info['cat_cols'],
    numerical_cols=data_args.column_info['num_cols'],
    sep_text_token_str=tokenizer.sep_token,
)

train_dataset.labels = train_df['LABEL'].values
test_dataset.labels = test_df['LABEL'].values
val_dataset.labels = val_df['LABEL'].values

train_dataset.numerical_feats = train_numerical_cols
val_dataset.numerical_feats = val_numerical_cols
test_dataset.numerical_feats = test_numerical_cols

num_labels = len(np.unique(train_dataset.labels))
print(num_labels)

config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
    )
tabular_config = TabularConfig(num_labels=num_labels,
                               cat_feat_dim=train_dataset.cat_feats.shape[1],
                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],
                               **vars(data_args))
config.tabular_config = tabular_config

model = AutoModelWithTabular.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        config=config,
        cache_dir=model_args.cache_dir
    )

"""### We need to define a task-specific way of computing relevant metrics:"""

import numpy as np
from scipy.special import softmax
from sklearn.metrics import (
    auc,
    precision_recall_curve,
    roc_auc_score,
    f1_score,
    confusion_matrix,
    matthews_corrcoef,
)

def calc_classification_metrics(p: EvalPrediction):
  pred_labels = np.argmax(list(p.predictions[0]), axis=1)
  pred_scores = softmax(list(p.predictions[0]), axis=1)[:, 1]
  labels = p.label_ids
  if len(np.unique(labels)) == 2:  # binary classification
      roc_auc_pred_score = roc_auc_score(labels, pred_scores)
      precisions, recalls, thresholds = precision_recall_curve(labels,pred_scores)
      fscore = (2 * precisions * recalls) / (precisions + recalls)
      fscore[np.isnan(fscore)] = 0
      ix = np.argmax(fscore)
      threshold = thresholds[ix].item()
      pr_auc = auc(recalls, precisions)
      tn, fp, fn, tp = confusion_matrix(labels, pred_labels, labels=[0, 1]).ravel()
      result = {'roc_auc': roc_auc_pred_score,
                'threshold': threshold,
                'pr_auc': pr_auc,
                'recall': recalls[ix].item(),
                'precision': precisions[ix].item(), 'f1': fscore[ix].item(),
                'tn': tn.item(), 'fp': fp.item(), 'fn': fn.item(), 'tp': tp.item()
     }
  else:
      acc = (pred_labels == labels).mean()
      f1 = f1_score(y_true=labels, y_pred=pred_labels)
      result = {
          "acc": acc,
          "f1": f1,
          "acc_and_f1": (acc + f1) / 2,
          "mcc": matthews_corrcoef(labels, pred_labels)
      }

  return result



trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=calc_classification_metrics)

trainer.train()

test_data = pd.read_csv('test.csv')
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=calc_classification_metrics)
eval_results = trainer.evaluate(test_dataset)

# Print the evaluation results
print(eval_results)